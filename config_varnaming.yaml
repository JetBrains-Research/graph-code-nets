model:
  encoder: "gcn"
  decoder: "transformer_decoder"
  gcn:
    out_channels: 512
    hidden_channels: 64
    num_layers: 1
  transformer_decoder:
    num_layers: 6
    d_model: 512
    dim_feedforward: 2048
    nhead: 8
    dropout: 0.1

vocabulary:
  type: 'spm'
  path: 'pretrained/vocab_bpe_10p.model'   # e.g. my_vocab.model
  max_token_length: 16  # In terms of (BPE) sub-tokens. 0.999 percentile word coverage

data:
  root: 'datasets/example_3_samples4'
  link: 'https://drive.google.com/u/0/uc?id=1GcpIFH5wD7TTnaemtdy7ieMaSK2T8HN2'
  cache_in_ram: True

trainer:
  max_epochs: 5
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: 1
  precision: 16

checkpoint:
  dir: 'checkpoints'
  top_k: 3

logger:  # wandb is used
  project: 'varnaming-training'
  entity: 'scaling-graph-nets'

train:
  learning_rate: 0.0001

  dataset:
    debug: False

  dataloader:
    batch_size: 64
    num_workers: 4
    prefetch_factor: 2
    pin_memory: True  # TODO investigate if has any effect
    persistent_workers: True

validation:
  dataset:
    debug: False

  dataloader:
    batch_size: 64
    num_workers: 4
    pin_memory: True
    persistent_workers: True

test:
  dataset:
    debug: False

  dataloader:
    batch_size: 64
    num_workers: 4
    pin_memory: True
    persistent_workers: True
