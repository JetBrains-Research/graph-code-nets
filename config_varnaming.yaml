seed: 1337

model:
  # emnedding.embedding_dim, gcn.out_channels and transformer_decoder.d_model must be the same
  encoder: "gcn"
  decoder: "transformer_decoder"
  embedding:
    embedding_dim: 128
  gcn:
    out_channels: 128
    hidden_channels: 64
    num_layers: 6
  transformer_decoder:
    num_layers: 6
    d_model: 128
    dim_feedforward: 2048
    nhead: 8
    dropout: 0.1
  debug: True

vocabulary:
  type: 'spm'
  path: 'pretrained/vocab_uni_10p_pad.model'   # e.g. my_vocab.model; unigram is more perfomance than bpe
  max_token_length: 18  # In terms of (BPE) sub-tokens. 0.999 percentile word coverage

data:
  root: 'datasets/java_small_preprocessed_pad'
  link: 'https://drive.google.com/u/0/uc?id=1GcpIFH5wD7TTnaemtdy7ieMaSK2T8HN2'
  cache_in_ram: False  # if True, must be used with dataloader.persistent_workers: True
  preprocessed: True  # with scripts/preprocess_varnaming_dataset.py
  max_node_count: 300  # 300: 0.52 percentile, 5000: 0.9972 percentile; bigger graphs could cause OOM and will be ignored

trainer:
  max_epochs: 10
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: 1
  val_check_interval: 9600  # ~8 times per train epoch (< 2500000/batch_size/4)
  precision: 16
  num_sanity_val_steps: 2
  auto_lr_find: True
  gradient_clip_val: 1.0
  limit_test_batches: 5800  # no more than test dataset size (e.g. for launching test() on train dataset)
  accumulate_grad_batches: 1

checkpoint:
  dir: 'checkpoints'
  top_k: 3

logger:  # wandb is used
  project: 'varnaming-training'
  entity: 'scaling-graph-nets'

train:
  learning_rate: 0.001
  lr_decay_gamma: 0.95

  dataset:
    debug: True

  dataloader:
    batch_size: 64
    num_workers: 4
    prefetch_factor: 2
    pin_memory: True  # TODO investigate if has any effect
    persistent_workers: True

validation:
  dataset:
    debug: True

  dataloader:
    batch_size: 64
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    mrr_k: 1
    acc_k: 1

test:
  dataset:
    debug: True

  dataloader:
    batch_size: 64
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    method: "greedy"
    gen_check_interval: 9600
    bandwidth: 5
    max_steps: 1000
    mrr_k: 1
    acc_k: 1
