seed: 1337

model:
  encoder: "gcn"
  decoder: "transformer_decoder"
  gcn:
    out_channels: 512
    hidden_channels: 64
    num_layers: 1
  transformer_decoder:
    num_layers: 6
    d_model: 512
    dim_feedforward: 2048
    nhead: 8
    dropout: 0.1

vocabulary:
  type: 'spm'
  path: 'pretrained/vocab_uni_10p_pad.model'   # e.g. my_vocab.model; unigram is more perfomance than bpe
  max_token_length: 18  # In terms of (BPE) sub-tokens. 0.999 percentile word coverage

data:
  root: 'datasets/java_small_preprocessed_pad'
  link: 'https://drive.google.com/u/0/uc?id=1GcpIFH5wD7TTnaemtdy7ieMaSK2T8HN2'
  cache_in_ram: False  # if True, must be used with dataloader.persistent_workers: True
  preprocessed: True  # with scripts/preprocess_varnaming_dataset.py
  max_node_count: 5000  # 0.9972 percentile; bigger graphs could cause OOM and will be ignored

trainer:
  max_epochs: 2
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: 1
  val_check_interval: 4800  # ~8 times per train epoch (< 2500000/batch_size/4)
  precision: 16
  num_sanity_val_steps: 1
  auto_lr_find: True

checkpoint:
  dir: 'checkpoints'
  top_k: 3

logger:  # wandb is used
  project: 'varnaming-training'
  entity: 'scaling-graph-nets'

train:
  learning_rate: 0.0001

  dataset:
    debug: True

  dataloader:
    batch_size: 64
    num_workers: 4
    prefetch_factor: 2
    pin_memory: True  # TODO investigate if has any effect
    persistent_workers: True

validation:
  dataset:
    debug: True

  dataloader:
    batch_size: 64
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    mrr_k: 1
    acc_k: 1

test:
  dataset:
    debug: False

  dataloader:
    batch_size: 64
    num_workers: 4
    pin_memory: True
    persistent_workers: True

  generation:
    method: "greedy"
    gen_check_interval: 9600
    bandwidth: 5
    max_steps: 1000
    mrr_k: 1
    acc_k: 1
