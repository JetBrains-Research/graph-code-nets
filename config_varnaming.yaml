model:
  encoder: "gcn"
  decoder: "transformer_decoder"
  gcn:
    out_channels: 512
    hidden_channels: 64
    num_layers: 1
  transformer_decoder:
    num_layers: 6
    d_model: 512
    dim_feedforward: 2048
    nhead: 8
    dropout: 0.1

vocabulary:
  type: 'spm'
  path: 'pretrained/vocab_bpe_10p.model'   # e.g. my_vocab.model
  max_token_length: 16  # In terms of (BPE) sub-tokens. 0.999 percentile word coverage

data:
  root: 'datasets/java_small/'
  link: 'https://drive.google.com/u/0/uc?id=1GcpIFH5wD7TTnaemtdy7ieMaSK2T8HN2'

trainer:
  max_steps: 100
  accelerator: gpu
  devices: 1

checkpoint:
  dir: 'checkpoints'
  top_k: 3

logger:  # wandb is used
  project: 'varnaming-training'

train:
  learning_rate: 0.0001

  dataset:
    debug: False

  dataloader:
    batch_size: 64
    num_workers: 4

validation:
  dataset:
    debug: False

  dataloader:
    batch_size: 64
    num_workers: 4

test:
  dataset:
    debug: False

  dataloader:
    batch_size: 64
    num_workers: 4
