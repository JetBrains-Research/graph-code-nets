paths:
  data: "../data"
  raw_data: "../data"
  vocab: "../vocab.txt"
model:
  configuration: "rggnn"
  base:
    hidden_dim: 128
    dropout_rate: 0.1
    num_edge_types: 24
  rnn:
    num_layers: 8
  ggnn:
    num_layers: 8
  gcn:
    num_layers: 8
  rggnn:
    num_layers: 8
  gatv2conv:
    num_layers: 8
data:
  batch_size: 64 # In samples
  max_sequence_length: 128
  drop_sequence_length: 512
  max_token_length: 8  # In terms of (BPE) sub-tokens.
training:
  max_steps: 100
  print_freq: 1000
  learning_rate: 0.0001
